{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing is now everywhere. From Google search, google translate, analyzing product reviews in ecommarce sites like Amazon or in doing sentiment analysis on particular topics in Twitter, using machine learning for nlp is becoming more important every minute. There's many popular ways to transform raw text to numbers to use in a machine learning model such as bag of words or tf-idf(term frequency inverse document frequency). \n",
    "\n",
    "The limitation with most of these methods is that they don't capture meaning. If we only count the number of times each word appears in the text (bag of words) or even weight it with number of times those words appear in the documents(tf-idf), we still do not have any notion about which words are truly similar or share similar meaning. Word Embedding is an unsupervised technique for capturing word meanings, understanding semantic relationships between them by creating representation for words. \n",
    "\n",
    "\n",
    "In this tutorial we will cover following things: \n",
    "\n",
    "* Definition of word embeddings\n",
    "* How to train our own word embeddings with gensim\n",
    "* Visualizing the word embeddings using TSNE\n",
    "* Using pre-trained word vectors\n",
    "* How to use word embeddings for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What are Word Embeddings?\n",
    "\n",
    "Word embeddings ( also called word vectors) are mappings from words to real valued vectors. The key idea is that each word will be represented by a numeric vector. It is a dense representation of words in a low dimensional vector space.\n",
    "\n",
    "If we simply want to represent a word by a vector, we can use the one-hot-representation. Here the length of the vector corresponds to the size of total vocabulary. Each vector will have a non-zero-entry(1) in its place in the vocabulary.\n",
    "\n",
    "For example if we have a 3 word vocabulary ```{book, king, queen}```, we can have 3 word vectors like this.\n",
    "\n",
    "```\n",
    "book = [1,0,0]\n",
    "king = [0,1,0]\n",
    "queen = [0,0,1]\n",
    "```\n",
    "However if we have one million words in our vocabulary then we have vectors of 1 million length with only one non-zero-entry which would be very sparse vectors. One-hot-representations work well for discrete values of categorical data well, but they can also be inefficient when the vectors are very sparse. This takes a huge space in memory and when it comes to computing with these vectors(addition, multiplication) etc it'd take a lot of time.\n",
    "\n",
    "On top of that, one-hot-representations do not preserve similarity or relationships between words. The vector for king and the vector for queen does not have any similarity between them. If we take the dot product between these two vectors we will end up with a zero even if both words appear in similar contexts. One-hot-representations do not preserve semantic relationships.\n",
    "\n",
    "The solution is to create a 'distributed representation' that preserves the notion of similarity in a low-dimensional space. Word embeddings create a low dimensional(length of 50,100,300 etc instead of 1M+) vector containing real numbers where the meaning of the words is distributed through all the dimensions in the vector instead of just having one non-zero entry. In practice the word vectors may look like this : \n",
    "\n",
    "```\n",
    "book = [.34,.56,.22]\n",
    "king = [.94,.75,.83]\n",
    "queen = [.92,.70,.78]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will go through a brief overview of the popular word embedding models. \n",
    "\n",
    "* ```Word2vec``` :  Word2Vec was released by a team of researchers at Google led by Tomas Mikolov in 2013. Since then many other researchers have also explained word2vec...[more here]\n",
    "\n",
    "\n",
    "* ```Glove``` : [Glove](https://nlp.stanford.edu/projects/glove/) (Global Vectors for Word Representation) was created by Stanford NLP group. They provide many pre-trained vectors of different dimensions(50,100,200 or 300 dimensions) trained on Wikipedia and Twitter corpus for general use. [more here]\n",
    "\n",
    "\n",
    "* ```FastText``` : [FastText](https://github.com/facebookresearch/fastText) is the latest word embedding library released by Facebook in 2016. Unlike Word2Vec and Glove, FastText treats words as composed of its character n-grams. Vectors created for one word is the sum of vectors created for its character n-grams. For example, the vector for orange might be the sum of the vectors of the character level bigrams like ```or|ra|an|ng|ge|e```  and trigrams like ```ora|ran|ang|nge|ge|e```. This helps to generate better representations for out-of-vocabulary words and rare words because even if the word does not appear in the corpus or appears very infrequently it can still have character n-grams shared with other words. FastText also released pre-trained word vectors for 294 languages trained on wikipedia corpus in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some idea about what word embeddings are we can easily train our own word embeddings using Gensim wrapper for [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) and visualize the embeddings using t-sne from scikit learn. \n",
    "\n",
    "The dataset we will use comes from [Project Gutenberg](https://www.gutenberg.org/).[Pride and Prejudice](https://en.wikipedia.org/wiki/Pride_and_Prejudice) is a famous romance novel written by [Jane Austen](https://en.wikipedia.org/wiki/Jane_Austen), a very well-known author from late 18th century. The book was first published in 1813. Later many modern movies and dramas have been made out of the book. \n",
    "\n",
    "We'll get the data from Project Gutenberg and preprocess the data for making it ready for the gensim word2vec model. Then we will train the word embeddings with gensim's word2vec.\n",
    "\n",
    "First we'll load all the libraries we may need. Here's a brief review of how each library will be used : \n",
    "\n",
    "* ```gensim``` : To create word vectors\n",
    "* ```requests``` : To get the data from Project Gutenberg\n",
    "* ```re``` : Regex for cleaning the dataset\n",
    "* ```numpy```, ```sklearn```,```matplotlib``` : For plotting the word vectors in 2 dimension with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Here we have a few helper function for collecting the text of the book from Project Gutenberg. The function ```get_book``` gets the pride and prejudice text from the project gutenberg website and discards some extra text for processing. \n",
    "\n",
    "```preprocess``` function keeps only the alpha-numeric characters and removes all special characters. It also makes the text lowercase and splits it into a list of sentences.\n",
    "\n",
    "```get_book_corpus``` function uses both of the helper functions ```get_book``` and ```preprocess``` to get the book text as a list of sentences. Then it breaks up the sentences into words for tokenization and returns the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pride_and_prejudice_url = 'http://www.gutenberg.org/files/1342/1342-0.txt'\n",
    "\n",
    "def get_book(url):\n",
    "    # gets the text from project gutenberg with a http request\n",
    "    raw = requests.get(url).text\n",
    "    # each book in project gutenberg has some metadata in the beginning so we discard them\n",
    "    start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",raw ).end()\n",
    "    # similarly there's some extra text in the end too, so we discard them\n",
    "    stop = re.search(r\"End of the Project Gutenberg EBook\", raw).start()\n",
    "    # we only keep the relevant text\n",
    "    text = raw[start:stop]\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess(sentence): \n",
    "    return re.sub('[^A-Za-z0-9.]+', ' ', sentence).lower().split(\".\")\n",
    "    \n",
    "\n",
    "def get_book_corpus(book_url):\n",
    "    book = get_book(book_url)\n",
    "    processed_book = preprocess(book)\n",
    "    corpus = [sentence.split(\" \") for sentence in processed_book]\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pride_and_prejudice = get_book_corpus(pride_and_prejudice_url)\n",
    "corpus = pride_and_prejudice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have the word tokenized version of the famous first two opening sentences from pride and prejudice with some extra header words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some uninformative words in text like blank space(\"\") and many stopwords like a, the etc which occur frequently but does not add much to the meaning of the sentences. It's not that big of a problem when we are training on millions of news articles and want to improve classification accuracy on something like google search, but here we are going to visualize the embeddings, so it's better if we can get a good vocabulary set. We get the final corpus after removing the stopwords with the help of [NLTK](http://www.nltk.org/), the de-facto libary for natural language processing in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english') + ['']\n",
    "\n",
    "final_corpus = []\n",
    "for sentence in corpus:\n",
    "    dummy = []\n",
    "    for word in sentence:\n",
    "        if word not in stopwords_list:\n",
    "            dummy.append(word)\n",
    "    final_corpus.append(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2Vec Model\n",
    "\n",
    "We'll briefly go over the details of the word2vec model. First we'll get familiar with some terminology like context words and center word. Let's consider an example sentence.\n",
    "\n",
    "\" I love ham and egg sandwich with coffee for breakfast.\"\n",
    "\n",
    "For each word in the sentence we can consider that word as the 'center' and the surrounding words can be considered as the context word. If we pick 'coffee' as the center word with a window size of 1 for the context words, then 'with' and 'for' are the context words. Similarly if we pick 'love' as the center word, then 'I' and 'ham' will be the context words. The window size simply determines number of words in the left and right of the center word should be considered as context words. With a window size of 2 instead of 1, context words for coffee becomes {sandwich, with} and {for, breakfast}.\n",
    "\n",
    "\n",
    "\n",
    "Word2Vec can be trained with two architectures, continuous bag of words(CBOW) and skip-gram model. CBOW predicts the current or center word given the window of the context words. The order of the context words does not impact the prediction. Skip-gram predicts the context words for a given window size given a center word. CBOW is faster to train but skip-gram is slightly better in predicting infrequent words.\n",
    "\n",
    "\n",
    "Gensim provides many parameters to control the training of the Word2Vec model.\n",
    "\n",
    "* ```sentences``` : An iterable containing the corpus for training.\n",
    "* ```sg```: 0 or 1. It defines the training algorithm. Default value is 0 and CBOW model is used. If we use 1 then skip-gram model is used.\n",
    "* ```size``` : The dimension of the embeddings that we specify. 300 will give us a 300 dimensional vector, 150 will give us a 150 dimensional vector for each word and so on.\n",
    "* ```window``` : Maximum distance between the current word and predicted word in a sentence.\n",
    "* ```min_count``` : Words with lower frequency than the min_count will be discarded.\n",
    "* ```alpha``` : Initial learning rate. Default is 0.025\n",
    "* ```seed``` : Random seed for getting the same results later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(final_corpus, size=300, window=8, min_count=10,seed=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using trained embeddings\n",
    "\n",
    "After the model has been trained, we can check the word vector for some particular word. Darcy is one of the main characters in pride and prejudice, so let's check his word vector. As expected, we have a 300 dimensional real valued vector for the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['darcy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how similar two words are with respect to occuring in the same context. Darcy and Elizabeth occurs together very frequently so their similarity is quite high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('darcy','elizabeth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see which words are most similar to Elizabeth. First one is Lydia, her sister."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('elizabeth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can check the similar words for darcy. The 'letter' refers to parts in the model where Darcy and Elizabeth have some heated discussions about a letter. Mr. Collins, Lydia and Mr. Bennet are other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('darcy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('bingley')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many words were included in the final vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which words were included in the vocabulary with the keys method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, there is a gensim.models.phrases module which lets us automatically detect phrases longer than one word. Using phrases,we can learn a word2vec model where “words” are actually multiword expressions, such as new_york_times or financial_crisis. So we can give it a try too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_transformer = gensim.models.Phrases(final_corpus)\n",
    "model2 = gensim.models.Word2Vec(bigram_transformer[final_corpus],size=300, window=6, min_count=90,seed = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.most_similar(\"collins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our embeddings, we can use them for visualization and even for text classification.\n",
    "\n",
    "\n",
    "# Visualizing word embeddings with TSNE \n",
    "\n",
    "TSNE is a visualization method frequently used for visualizing high dimensional vectors in 2D or 3D space. Note that the axis in the TSNE visualization doesn't mean anything, rather the relative positions of the vectors show which vectors are closer in the high dimensional space. We have created embeddings of 300 dimensions, but we can visualize them with TSNE in the 2D space. We use a limit of 400 here because visualizing all the words can look a bit cluttered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "limit = 400\n",
    "vector_dim = 300\n",
    "\n",
    "# Getting tokens and vectors\n",
    "words = []\n",
    "embedding = np.array([])\n",
    "i = 0\n",
    "for word in model.wv.vocab.keys():\n",
    "    # Break the loop if limit exceeds \n",
    "    if i == limit: break\n",
    "\n",
    "    # Getting token \n",
    "    words.append(word)\n",
    "\n",
    "    # Appending the vectors \n",
    "    embedding = np.append(embedding, model[word])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Reshaping the embedding vector \n",
    "embedding = embedding.reshape(limit, vector_dim)\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "# Creating the tsne plot [Warning: will take time]\n",
    "tsne = TSNE(perplexity=30.0, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "low_dim_embedding = tsne.fit_transform(embedding)\n",
    "\n",
    "# Finally plotting and saving the fig \n",
    "plot_with_labels(low_dim_embedding, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using pre-trained word embeddings \n",
    "\n",
    "\n",
    "In this part we'll demonstrate how to use download, load and use Google's pretrained word vectors in gensim. These word vectors are 300 dimensional and they were trained on a corpus of Google News Dataset with around 100 billion words. They have word vectors for a vocabulary of around 3 million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a short demonstration of text classification of word embeddings we'll use a small dataset from Kaggle containing recent 3000 tweets from Hilary Clinton and Donald Trump that were collected during the election. The dataset is named [\"Clinton-Trump Tweets\"](https://www.kaggle.com/benhamner/clinton-trump-tweets) and can be collected from Kaggle. The classification problem would be to find whether Hilary or Trump tweeted some particulr tweet given the tweet text. In other words it's a binary classification problem given some text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"tweets.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
