{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is now everywhere. From Google search, Google translate, analyzing product reviews in e-commerce sites like Amazon or in doing sentiment analysis on particular topics in Twitter, using machine learning for NLP is becoming more important every minute. There's many popular ways to transform raw text to numbers to use in a machine learning model such as bag-of-words or Term Frequency Inverse Document Frequency (TF-IDF). \n",
    "\n",
    "The limitation with most of these methods is that they don't capture meaning. If you only count the number of times each word appears in the text (bag-of-words) or even weight it with number of times those words appear in the documents (TF-IDF), you still do not have any notion about which words are truly similar or share similar meaning. Word Embedding is an unsupervised technique for capturing word meanings, understanding semantic relationships between them by creating representation for words. \n",
    "\n",
    "This tutorial will cover the following topics: \n",
    "\n",
    "* Definition of word embeddings\n",
    "* How to train our own word embeddings with `gensim`\n",
    "* Visualizing the word embeddings using t-SNE\n",
    "* Using pre-trained word vectors\n",
    "* How to use word embeddings for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What Are Word Embeddings? (NEED TO REWRITE USING MORE DIAGRAMS)\n",
    "\n",
    "Word embeddings (also called \"word vectors\") are mappings from words to real valued vectors. The key idea is that each word will be represented by a numeric vector. It is a dense representation of words in a low dimensional vector space.\n",
    "\n",
    "If you simply want to represent a word by a vector, you can use the one-hot-representation. Here, the length of the vector corresponds to the size of total vocabulary. Each vector will have a non-zero-entry(1) in its place in the vocabulary. For example, if you have a three-word vocabulary ```{book, king, queen}```, the three word vectors will look like this:\n",
    "\n",
    "```\n",
    "book = [1,0,0]\n",
    "king = [0,1,0]\n",
    "queen = [0,0,1]\n",
    "```\n",
    "\n",
    "However, if you have one million words in your vocabulary, then you have vectors of 1 million length with only one non-zero-entry which would be very sparse vectors. One-hot-representations work well for discrete values of categorical data well, but they can also be inefficient when the vectors are very sparse. This takes a huge space in memory and when it comes to computing with these vectors (that is, doing operations such as addition, multiplication, ...), it'd take a lot of time.\n",
    "\n",
    "On top of that, one-hot-representations do not preserve similarity or relationships between words. The vector for `king` and the vector for `queen` does not have any similarity between them. If you take the dot product between these two vectors, you will end up with a zero even if both words appear in similar contexts. \n",
    "\n",
    "<!--- One-hot-representations do not preserve semantic relationships. = sort of a repetition of the similarity/relationship between words?--->\n",
    "\n",
    "The solution to this problem is then to create a \"distributed representation\" that preserves the notion of similarity in a low-dimensional space. Word embeddings create a low dimensional(length of 50,100,300 etc instead of 1M+ <!-- where to these numbers come from? --->) vector containing real numbers where the meaning of the words is distributed through all the dimensions in the vector instead of just having one non-zero entry. \n",
    "\n",
    "With this solution in place, your word vectors could look like this : \n",
    "\n",
    "```\n",
    "book = [.34,.56,.22]\n",
    "king = [.94,.75,.83]\n",
    "queen = [.92,.70,.78]\n",
    "```\n",
    "\n",
    "\n",
    "<!-- I'm missing the wider context of word embeddings here. \n",
    "\n",
    "* Mainly the link to the history (why and how people moved to using word embeddings and what the first approaches were. : distributional semantics\n",
    "\n",
    "* Link to the broader context of NLP: frequency based vs prediction based embeddings\n",
    "\n",
    "https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/ \n",
    "term \"word embedding\" might not completely be applicable: http://ruder.io/word-embeddings-1/: \"Bengio et al. coin the term word embeddings in 2003 and train them in a neural language model jointly with the model's parameters.\" \n",
    "https://arxiv.org/abs/1705.01509 \n",
    "\n",
    "* Link from low dimensional vectors to the more advanced research that has been done and that is reflected in the next markdown cell\n",
    "---> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Word Embedding Models\n",
    "\n",
    "\n",
    "The rest of this tutorial will provide you with a brief overview of the popular word embedding models and you'll learn more about some popular software for training and using word embeddings:\n",
    "\n",
    "* ```Word2vec``` :  Word2Vec was released by a team of researchers at Google led by Tomas Mikolov in 2013. Since then many other researchers have also explained word2vec...[more here]\n",
    "\n",
    "\n",
    "* ```Glove``` : [Glove](https://nlp.stanford.edu/projects/glove/) (Global Vectors for Word Representation) was created by Stanford NLP group. They provide many pre-trained vectors of different dimensions(50,100,200 or 300 dimensions) trained on Wikipedia and Twitter corpus for general use. [more here]\n",
    "\n",
    "\n",
    "* ```FastText``` : [FastText](https://github.com/facebookresearch/fastText) is the latest word embedding library released by Facebook in 2016. Unlike Word2Vec and Glove, FastText treats words as composed of its character n-grams. Vectors created for one word is the sum of vectors created for its character n-grams. For example, the vector for orange might be the sum of the vectors of the character level bigrams like ```or|ra|an|ng|ge|e```  and trigrams like ```ora|ran|ang|nge|ge|e```. This helps to generate better representations for out-of-vocabulary words and rare words because even if the word does not appear in the corpus or appears very infrequently it can still have character n-grams shared with other words. FastText also released pre-trained word vectors for 294 languages trained on wikipedia corpus in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some idea about what word embeddings are, you can easily train our own word embeddings using Gensim wrapper for [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html); Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. After, you'll visualize the embeddings using `t-sne` from `scikit-learn`. \n",
    "\n",
    "The dataset that you will use comes from [Project Gutenberg](https://www.gutenberg.org/). [Pride and Prejudice](https://en.wikipedia.org/wiki/Pride_and_Prejudice) is a famous romance novel written by [Jane Austen](https://en.wikipedia.org/wiki/Jane_Austen), a very well-known author from late 18th century. The book was first published in 1813. Later, many modern movies and dramas have been made out of the book. \n",
    "\n",
    "![ ](images/ja.jpg)\n",
    "\n",
    "You'll get the data from Project Gutenberg and preprocess the data for making it ready for the `gensim` word2vec model. Then we will train the word embeddings with `gensim`'s word2vec.\n",
    "\n",
    "First, you'll load all the libraries that you may need. Here's a brief review of how each library will be used : \n",
    "\n",
    "* `gensim` : To create word vectors\n",
    "* `requests` : To get the data from Project Gutenberg\n",
    "* `re` : Regex for cleaning the dataset\n",
    "* `numpy`, `sklearn`, and `matplotlib` : For plotting the word vectors in 2 dimensions with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Here, you'll need to make some helper functions so that you can collect the text of the book from Project Gutenberg. \n",
    "* The function `get_book()` gets the pride and prejudice text from the Project Gutenberg website and discards some extra text for processing. \n",
    "\n",
    "* The `preprocess()` function keeps only the alpha-numeric characters and removes all special characters. It also makes the text lowercase and splits it into a list of sentences.\n",
    "\n",
    "* The `get_book_corpus()` function uses both of the helper functions `get_book()` and `preprocess()` to get the book text as a list of sentences. Then it breaks up the sentences into words for tokenization and returns the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pride_and_prejudice_url = 'http://www.gutenberg.org/files/1342/1342-0.txt'\n",
    "\n",
    "def get_book(url):\n",
    "    # gets the text from project gutenberg with a http request\n",
    "    raw = requests.get(url).text\n",
    "    # each book in project gutenberg has some metadata in the beginning so we discard them\n",
    "    start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",raw ).end()\n",
    "    # similarly there's some extra text in the end too, so we discard them\n",
    "    stop = re.search(r\"End of the Project Gutenberg EBook\", raw).start()\n",
    "    # we only keep the relevant text\n",
    "    text = raw[start:stop]\n",
    "    return text\n",
    "\n",
    "def preprocess(sentence): \n",
    "    return re.sub('[^A-Za-z0-9.]+', ' ', sentence).lower().split(\".\")\n",
    "    \n",
    "def get_book_corpus(book_url):\n",
    "    book = get_book(book_url)\n",
    "    processed_book = preprocess(book)\n",
    "    corpus = [sentence.split(\" \") for sentence in processed_book]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pride_and_prejudice = get_book_corpus(pride_and_prejudice_url)\n",
    "corpus = pride_and_prejudice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you have the word tokenized version of the famous first two opening sentences from \"Pride and Prejudice\" with some extra header words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(corpus[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some uninformative words in text like blank space (`\"\"`) and many stopwords like \"a\", \"the\", etc. These words occur frequently but do not add much to the meaning of the sentences. In essence, it's not a big problem when you are training on millions of news articles and want to improve classification accuracy on something like Google Search. However, in this case, you are going to visualize the embeddings, so it's better if you can get a good vocabulary set. You get the final corpus after removing the stopwords with the help of [NLTK](http://www.nltk.org/), the de-facto libary for Natural Language Processing (NLP) in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english') + ['']\n",
    "\n",
    "final_corpus = []\n",
    "for sentence in corpus:\n",
    "    dummy = []\n",
    "    for word in sentence:\n",
    "        if word not in stopwords_list:\n",
    "            dummy.append(word)\n",
    "    final_corpus.append(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2Vec Model\n",
    "\n",
    "In this section, you'll briefly go over the details of the word2vec model. First you'll get familiar with some terminology like \"context words\",\"center word\" and \"window size\". \n",
    "\n",
    "Let's consider an example sentence like the following: \"I love ham and egg sandwich with coffee for breakfast.\"\n",
    "\n",
    "For each word in the sentence, you can consider that word as the 'center' and the surrounding words as the context words. If you pick 'coffee' as the center word with a window size of 1 for the context words, then 'with' and 'for' are the context words. Similarly, if you pick 'love' as the center word, then 'I' and 'ham' will be the context words. The window size is simply the number of words to the left and right of the center word that should be considered as context words. With a window size of 2 instead of 1, the context words for \"coffee\" become {sandwich, with} and {for, breakfast}.\n",
    "\n",
    "Word2Vec can be trained with two architectures, namely, the Continuous Bag of Words(CBOW) and skip-gram model. On the one hand, CBOW predicts the current or center word given the window of the context words. The order of the context words does not impact the prediction. Skip-gram, on the other hand, predicts the context words for a given window size given a center word. CBOW is faster to train but skip-gram is slightly better in predicting infrequent words.\n",
    "\n",
    "Gensim provides many parameters to control the training of the Word2Vec model.\n",
    "\n",
    "* `sentences` : An iterable containing the corpus for training.\n",
    "* `sg`: 0 or 1. It defines the training algorithm. Default value is 0 and CBOW model is used. If you use 1 then skip-gram model is used.\n",
    "* `size` : The dimension of the embeddings that you specify. 300 will give us a 300 dimensional vector, 150 will give us a 150 dimensional vector for each word and so on.\n",
    "* `window` : Maximum distance between the current word and predicted word in a sentence.\n",
    "* `min_count` : Words with lower frequency than the min_count will be discarded.\n",
    "* `alpha` : Initial learning rate. Default is 0.025\n",
    "* `seed` : Random seed for getting the same results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(final_corpus, size=300, window=8, min_count=10,seed=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Trained Embeddings\n",
    "\n",
    "After the model has been trained, you can check the word vector for some particular word. \"Darcy\" is one of the main characters in the novel \"Pride and Prejudice\", so let's check his word vector. As expected, you have a 300 dimensional real-valued vector for the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model['darcy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check how similar two words are with respect to occuring in the same context with the `similarity()` method. \"Darcy\" and \"Elizabeth\" occur together frequently, which explains why their similarity is quite high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.similarity('darcy','elizabeth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see which words are most similar to Elizabeth with the help of the `most_similar()` method. First one is Lydia, her sister."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('elizabeth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can check the similar words for \"Darcy\". The \"letter\" refers to parts in the novel where two of the main characters, Darcy and Elizabeth, have some heated discussions about a letter. \"Mr. Collins\", \"Lydia\" and \"Mr. Bennet\" refer to other characters of the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('darcy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('bingley')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check how many words were included in the final vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check which words were included in the vocabulary with the `keys()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, there is a `gensim.models.phrases` module which lets you automatically detect phrases longer than one word. Using the `phrases` module, you can learn where \"words\" are actually multi-word expressions, such as `new_york_times` or `financial_crisis`. You can give this a try for the \"Pride and Prejudice\" novel, too: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_transformer = gensim.models.Phrases(final_corpus)\n",
    "model2 = gensim.models.Word2Vec(bigram_transformer[final_corpus],size=300, window=6, min_count=90,seed = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.wv.most_similar(\"collins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- small discussion of the results of the phrases module ---> \n",
    "With your embeddings, you can create visualizations and even perform text classifications.\n",
    "\n",
    "\n",
    "# Visualizing Word Embeddings With t-SNE \n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a visualization method that is frequently used for visualizing high dimensional vectors in 2D or 3D space. \n",
    "\n",
    "Note that the axis in the t-SNE visualization doesn't mean anything, rather the relative positions of the vectors show which vectors are closer in the high-dimensional space. <!--- I don't understand where \"axis\" comes from? ---> \n",
    "\n",
    "You have created embeddings of 300 dimensions, but you can visualize them with t-SNE in the 2D space. You use a limit of 400 here because visualizing all the words can look a bit cluttered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit = 400\n",
    "vector_dim = 300\n",
    "\n",
    "# Get tokens and vectors\n",
    "words = []\n",
    "embedding = np.array([])\n",
    "i = 0\n",
    "for word in model.wv.vocab.keys():\n",
    "    # Break the loop if limit exceeds \n",
    "    if i == limit: break\n",
    "\n",
    "    # Get token \n",
    "    words.append(word)\n",
    "\n",
    "    # Append the vectors \n",
    "    embedding = np.append(embedding, model[word])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Reshape the embedding vector \n",
    "embedding = embedding.reshape(limit, vector_dim)\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "# Create the t-SNE plot [Warning: will take time]\n",
    "tsne = TSNE(perplexity=30.0, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "low_dim_embedding = tsne.fit_transform(embedding)\n",
    "\n",
    "# Plot and save the figure\n",
    "plot_with_labels(low_dim_embedding, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pre-trained word embeddings \n",
    "\n",
    "\n",
    "In this part, you'll see how you can download, load and use Google's pretrained word vectors in `gensim`. These word vectors are 300 dimensional and they were trained on a corpus of Google News Dataset with around 100 billion words. They have word vectors for a vocabulary of around 3 million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained word embeddings for classification with Keras :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section we'll participate in 2017 Halloween themed Kaggle Competition [\"Spooky Author Identification\"](https://www.kaggle.com/c/spooky-author-identification) where the challenge is to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft using word embeddings and keras.\n",
    "\n",
    "![](images/hl.jpg)\n",
    "\n",
    "It's a multiclass classification problem with 3 classes - EAP, HLP and MWS , short form of the author names. The metric for evaluation is multiclass logarithmic loss/categorical crossentropy, a very common metric for multiclass classification models. \n",
    "\n",
    "The competition dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer, so you may notice the odd non-sentence here and there. The objective is to accurately identify the author of the sentences in the test set. The dataset can be downloaded from [Kaggle](https://www.kaggle.com/c/spooky-author-identification/data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
