Check in days : 

* Friday  7 th July 

  * Seeing all videos
  * Getting the 'gist' of word embedding
  * Writing the overview, definition, historically popular word embeddings

* Wednesday 12 July :

  * Using Pre-trained word vector part (around 5 days should be enough to do it)

  * Research gensim.

    ​

    ​

#### Article : Word Embedding tutorial

**Overview**

**Definition of word embeddings**

As a mapping from word to numeric vectors 

**Historically popular pre-trained word embeddings :** 

* Word2vec by Google
* Glove by Stanford NLP
* FastText by Facebook

**Using pre-trained word embeddings** 

(Either Glove/FastText, I'm thinking about FastText but Glove has more available tutorials in keras) :

 * Downloading them
 * Showing pairwise distance between some words
 * Nearest neighbor of some words
 * Visualizing them with t-sne in scikit learn
 * Optional ( Use keras from tensorflow for doing everything and visualize them in tensorboard) (Unsure how to make it work)

**Using pre-trained word embeddings for classification in Keras :** 

* Doing text classification without word embeddings with bag of words/tf-idf 
* Doing text classification with word embedding and compare.

**Conclusion** 