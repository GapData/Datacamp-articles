#### Article : Word Embedding tutorial

**Overview**

**Definition of word embeddings**

As a mapping from word to numeric vectors 

**Historically popular pre-trained word embeddings :** 

* Word2vec by Google
* Glove by Stanford NLP
* FastText by Facebook

**Using pre-trained word embeddings** 

Brief mention and resources.

**Using pre-trained word embeddings for classification in Keras :** 

* Dataset overview, 
* Preprocessing with keras tokenizer class
* Converting the text to embeddings
* Creating a 1D convolutional model/LTSM network and use it for text classification
* Checking it against a baseline like multinomial bias.

**Conclusion** 